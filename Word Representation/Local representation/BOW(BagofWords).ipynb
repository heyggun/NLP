{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a852ee",
   "metadata": {},
   "source": [
    "# BOW (Bag of words)\n",
    "\n",
    "    - local word representation이면서 count-based representation\n",
    "    - 단어의 등장 순서를 고려하지 않고 빈도(frequency)에 집중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "438475a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10d5027",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3b7800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(document):\n",
    "    document = document.replace('.','')\n",
    "    tokenized_document = okt.morphs(document)\n",
    "    \n",
    "    word_to_idx = {}\n",
    "    bow = []\n",
    "    \n",
    "    for word in tokenized_document:\n",
    "        if word not in word_to_idx.keys():\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "            bow.insert(len(word_to_idx)-1, 1)\n",
    "\n",
    "        else:\n",
    "            idx = word_to_idx.get(word)\n",
    "            bow[idx] = bow[idx]+1\n",
    "            \n",
    "    return word_to_idx, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f677e909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc1 = '정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.'\n",
    "vocab, bow = bag_of_words(doc1)\n",
    "print(vocab)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41b19f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc2 = '소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\n",
    "\n",
    "vocab, bow = bag_of_words(doc2)\n",
    "print(vocab)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a8c41e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "[1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc3 = doc1+ ' ' + doc2\n",
    "\n",
    "vocab, bow = bag_of_words(doc3)\n",
    "print(vocab)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad011e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59c39d98",
   "metadata": {},
   "source": [
    "    - doc3은 doc1, doc2 의 단어들을 모두 포함하고 있음\n",
    "    - bow는 9여러 문서의 단어 집합을 합친 뒤 해당 단어 집합에 대한 각 문서의 bow를 구하기도 함\n",
    "    - doc3 에 대한 단어 집합을 기준으로 doc1, doc2의 BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f3fb835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " doc3 단어 집합에 대한 doc1 의 Bow : \n",
      " [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " doc3 단어 집합에 대한 doc2 의 Bow : \n",
      " [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "vocab_doc1 = [0 for _ in range(len(vocab))]\n",
    "vocab_doc2 = [0 for _ in range(len(vocab))]\n",
    "\n",
    "for t1 in okt.morphs(doc1):\n",
    "    if t1 in vocab.keys():\n",
    "        vocab_doc1[vocab.get(t1)] +=1\n",
    "\n",
    "for t2 in okt.morphs(doc2):\n",
    "    if t2 in vocab.keys():\n",
    "        vocab_doc2[vocab.get(t2)] +=1\n",
    "        \n",
    "\n",
    "print(f\" doc3 단어 집합에 대한 doc1 의 Bow : \\n {vocab_doc1}\")\n",
    "print(f\" doc3 단어 집합에 대한 doc2 의 Bow : \\n {vocab_doc2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d36f155",
   "metadata": {},
   "source": [
    "    - 예를 들어 '물가상승률' 은 doc3에서 index 4에 해당하는데\n",
    "    doc1에서는 2회 등장, doc2 에서는 1회 등장했다.\n",
    "    \n",
    "    - BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법으로,\n",
    "    주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지 판단하는 작업에 쓰인다.\n",
    "    - 즉 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰인다.\n",
    "    \n",
    "    예를 들어 '달리기', '체력', '근력'의 단어가 자주 등장하면 해당 문서를 '체육' 관련 문서로, '미분', '방정식',' 부등식' 과 같은 단어가 자주 등장하면 '수학' 관련 문서로 분류할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad621057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55040a0b",
   "metadata": {},
   "source": [
    "## CountVectorizer 클래스로 BoW 만들기\n",
    "\n",
    "    - sklearn에서 단어의 빈도를 count 해서 vector로 만드는 CountVectorizer 클래스 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9b4e011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bow vector -> [[1 1 2 1 2 1]]\n",
      " vocabulary -> {'because': 0, 'know': 1, 'love': 2, 'want': 3, 'you': 4, 'your': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print(f\" bow vector -> {vector.fit_transform(corpus).toarray()}\")\n",
    "print(f\" vocabulary -> {dict(sorted(vector.vocabulary_.items(), key = lambda x: x[1]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33487668",
   "metadata": {},
   "source": [
    "    -> CountVectorizer는 기본적으로 길이가 2이상인 문자에 대해서만 token으로 인식해서, 'I' 사라짐\n",
    "    CountVectorizer는 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화로 BoW를 만듦\n",
    "    영어는 띄어쓰기 만으로 토큰화가 수행되도 상관없지만 한국어는 조사 등의 이유로 제대로 BoW가 만들어지기 어려움\n",
    "    예를 들어 '물가상승률은' 과 '물가상승률과'를 각자 다른 인덱스에서 1이라는 빈도를 갖게 되버림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343a119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b890756",
   "metadata": {},
   "source": [
    "## 불용어(stop words) 제거 후 BoW 만들기\n",
    "\n",
    "    - 불용어는 자연어처리에서 별 의미를 갖지 못하는 단어들\n",
    "    - BoW를 사용한다는 것은 문서내에서 단어가 얼마나 자주 등장했는지를 보고 싶다는 것으로,\n",
    "    각 단어에 대해 수치화하겠다는 것은 텍스트 내의 어떤 단어들이 중요한지 보고 싶다는 의미가 함축된 것으로\n",
    "    BoW를 만들 때 불용어를 제거하는 일은 자연어 처리의 정확도를 높일 수 있는 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b629b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f5c3a",
   "metadata": {},
   "source": [
    "    <사용자가 직접 불용어 넣기>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf894b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bow : [[1 1 1 1 1]]\n",
      " vocabulary : {'everything': 0, 'family': 1, 'important': 2, 'it': 3, 'thing': 4}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vec = CountVectorizer(stop_words=['the','a','an','is','not'])\n",
    "print(f\" bow : {vec.fit_transform(text).toarray()}\")\n",
    "print(f\" vocabulary : {dict(sorted(vec.vocabulary_.items(), key=lambda x: x[1]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd102d",
   "metadata": {},
   "source": [
    "    < CountVectorizer에서 지원하는 불용어 사용>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48a442a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bow : [[1 1 1]]\n",
      " vocabulary : {'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vec =  CountVectorizer(stop_words='english')\n",
    "print(f\" bow : {vec.fit_transform(text).toarray()}\")\n",
    "print(f\" vocabulary : {dict(sorted(vec.vocabulary_.items(), key=lambda x: x[1]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4710f",
   "metadata": {},
   "source": [
    "    <NLTK에서 지원하는 불용어 사용>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e514d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bow : [[1 1 1 1]]\n",
      " vocabulary : {'everything': 0, 'family': 1, 'important': 2, 'thing': 3}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words('english')\n",
    "vec =  CountVectorizer(stop_words=stop_words)\n",
    "print(f\" bow : {vec.fit_transform(text).toarray()}\")\n",
    "print(f\" vocabulary : {dict(sorted(vec.vocabulary_.items(), key=lambda x: x[1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8147c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
