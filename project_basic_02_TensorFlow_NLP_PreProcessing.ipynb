{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow.Keras로 자연어 전처리 하기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Integer Encoding 하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터도 내려 받는다.\n",
    "# 주의: 느림.\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. 기본적인 Integer Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터.\n",
    "my_sentences = [\"The economic slowdown is becoming more severe\",\n",
    "           \"The movie was simply awesome\",\n",
    "           \"I like cooking my own food\",\n",
    "           \"Samsung is announcing a new technology\",\n",
    "           \"Machine Learning is an example of awesome technology\",\n",
    "           \"All of us were excited at the movie\",\n",
    "           \"We have to do more to reverse the economic slowdown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['economic', 'slowdown', 'becoming', 'severe']\n",
      "['movie', 'simply', 'awesome']\n",
      "['like', 'cooking', 'food']\n",
      "['samsung', 'announcing', 'new', 'technology']\n",
      "['machine', 'learning', 'example', 'awesome', 'technology']\n",
      "['us', 'excited', 'movie']\n",
      "['reverse', 'economic', 'slowdown']\n"
     ]
    }
   ],
   "source": [
    "my_corpus = []\n",
    "for a_sentence in my_sentences:\n",
    "    a_sentence = a_sentence.lower()\n",
    "    a_sentence = re.sub(r'\\W',' ',a_sentence)            # 특수 문자는 space로 대체. \n",
    "    a_sentence = re.sub(r'\\s+',' ',a_sentence)           # 잉여 space 제거.\n",
    "    a_sentence = [x for x in nltk.word_tokenize(a_sentence) if x not in stopwords.words('english')] \n",
    "    print(a_sentence)\n",
    "    my_corpus.append(a_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer Encoding 학습 실시!\n",
    "my_tokenizer = Tokenizer()\n",
    "my_tokenizer.fit_on_texts(my_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'economic': 1, 'slowdown': 2, 'movie': 3, 'awesome': 4, 'technology': 5, 'becoming': 6, 'severe': 7, 'simply': 8, 'like': 9, 'cooking': 10, 'food': 11, 'samsung': 12, 'announcing': 13, 'new': 14, 'machine': 15, 'learning': 16, 'example': 17, 'us': 18, 'excited': 19, 'reverse': 20}\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력.\n",
    "# word -> index 사전이 만들어 졌다.\n",
    "# 빈도수 대로 index가 주어진다.\n",
    "print(my_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('economic', 2), ('slowdown', 2), ('becoming', 1), ('severe', 1), ('movie', 2), ('simply', 1), ('awesome', 2), ('like', 1), ('cooking', 1), ('food', 1), ('samsung', 1), ('announcing', 1), ('new', 1), ('technology', 2), ('machine', 1), ('learning', 1), ('example', 1), ('us', 1), ('excited', 1), ('reverse', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 도수표 출력.\n",
    "print(my_tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 6, 7], [3, 8, 4], [9, 10, 11], [12, 13, 14, 5], [15, 16, 17, 4, 5], [18, 19, 3], [20, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 학습된 객체를 사용하여 문장들을 integer encoding 한다.\n",
    "print(my_tokenizer.texts_to_sequences(my_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Vocabulary 크기를 제한하고 다시한번 실행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10개의 단어를 사용해 본다.\n",
    "# 인덱스 0은 padding의 용도로 사용될 것이기 때문에 +1 해야한다.\n",
    "vocab_size = 10\n",
    "my_tokenizer = Tokenizer(num_words = vocab_size + 1) \n",
    "my_tokenizer.fit_on_texts(my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'economic': 1, 'slowdown': 2, 'movie': 3, 'awesome': 4, 'technology': 5, 'becoming': 6, 'severe': 7, 'simply': 8, 'like': 9, 'cooking': 10, 'food': 11, 'samsung': 12, 'announcing': 13, 'new': 14, 'machine': 15, 'learning': 16, 'example': 17, 'us': 18, 'excited': 19, 'reverse': 20}\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력.\n",
    "# word -> index 사전이 만들어 졌다.\n",
    "# 빈도수 대로 index가 주어진다.\n",
    "# 주의: Top 10 단어로 제약을 두지는 않고 모두 보여준다!!!\n",
    "print(my_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('economic', 2), ('slowdown', 2), ('becoming', 1), ('severe', 1), ('movie', 2), ('simply', 1), ('awesome', 2), ('like', 1), ('cooking', 1), ('food', 1), ('samsung', 1), ('announcing', 1), ('new', 1), ('technology', 2), ('machine', 1), ('learning', 1), ('example', 1), ('us', 1), ('excited', 1), ('reverse', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 도수표 출력.\n",
    "# 주의: Top 10 단어로 제약을 두지는 않고 모두 보여준다!!!\n",
    "print(my_tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 6, 7], [3, 8, 4], [9, 10], [5], [4, 5], [3], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 학습된 객체를 사용하여 문장들을 integer encoding 한다.\n",
    "# 이제는 Top 10 단어로 제약이 적용된다!!!\n",
    "my_corpus_encoded = my_tokenizer.texts_to_sequences(my_corpus)\n",
    "print(my_corpus_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약에 word_index와 word_counts에도 제약이 적용되기를 원한다면 다음과 같이 코딩이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = Tokenizer() # num_words를 여기서는 지정하지 않은 상태\n",
    "my_tokenizer.fit_on_texts(my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'economic': 1, 'slowdown': 2, 'movie': 3, 'awesome': 4, 'technology': 5, 'becoming': 6, 'severe': 7, 'simply': 8, 'like': 9, 'cooking': 10}\n",
      "OrderedDict([('economic', 2), ('slowdown', 2), ('becoming', 1), ('severe', 1), ('movie', 2), ('simply', 1), ('awesome', 2), ('like', 1), ('cooking', 1), ('technology', 2)])\n",
      "[[1, 2, 6, 7], [3, 8, 4], [9, 10], [5], [4, 5], [3], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "words_to_delete = [a_word for (a_word,idx) in my_tokenizer.word_index.items() if idx >= vocab_size + 1] # 인덱스가 10 초과인 단어들의 리스트!\n",
    "for a_word in words_to_delete:\n",
    "    del my_tokenizer.word_index[a_word]  # 잉여 단어 삭제.\n",
    "    del my_tokenizer.word_counts[a_word] # 잉여 단어 삭제.\n",
    "print(my_tokenizer.word_index)\n",
    "print(my_tokenizer.word_counts)\n",
    "print(my_tokenizer.texts_to_sequences(my_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 잉여 단어 (OOV)는 그대로 무시하게 되는데, 이들에게 인덱스를 부여할 수도 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10개의 단어를 사용해 본다.\n",
    "# 인덱스 0은 padding의 용도로 사용될 것이기 때문에 +1 해야한다.\n",
    "# 인덱스 1은 OOV (out of vocabulary)를 나타내는 용도로 사용될 것이기 때문에 +1 해야 한다.\n",
    "vocab_size = 10\n",
    "my_tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')    # 총 +2 이다.\n",
    "my_tokenizer.fit_on_texts(my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV의 인덱스 : 1\n"
     ]
    }
   ],
   "source": [
    "print('OOV의 인덱스 : {}'.format(my_tokenizer.word_index['OOV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OOV': 1, 'economic': 2, 'slowdown': 3, 'movie': 4, 'awesome': 5, 'technology': 6, 'becoming': 7, 'severe': 8, 'simply': 9, 'like': 10, 'cooking': 11, 'food': 12, 'samsung': 13, 'announcing': 14, 'new': 15, 'machine': 16, 'learning': 17, 'example': 18, 'us': 19, 'excited': 20, 'reverse': 21}\n"
     ]
    }
   ],
   "source": [
    "# 모든 인덱스는 OOV 때문에 +1된 상황이다.\n",
    "print(my_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 7, 8], [4, 9, 5], [10, 11, 1], [1, 1, 1, 6], [1, 1, 1, 5, 6], [1, 1, 4], [1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "# 학습된 객체를 사용하여 문장들을 integer encoding 한다.\n",
    "# 이제는 Top 10 단어로 제약이 적용되며 잉여 단어 (OOV)는 인덱스 1로 표현이 된다.\n",
    "print(my_tokenizer.texts_to_sequences(my_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Padding 하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 6, 7], [3, 8, 4], [9, 10], [5], [4, 5], [3], [1, 2]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus_encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  6,  7],\n",
       "       [ 0,  3,  8,  4],\n",
       "       [ 0,  0,  9, 10],\n",
       "       [ 0,  0,  0,  5],\n",
       "       [ 0,  0,  4,  5],\n",
       "       [ 0,  0,  0,  3],\n",
       "       [ 0,  0,  1,  2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default padding은 \"pre\"이다.\n",
    "my_corpus_padded = pad_sequences(my_corpus_encoded, padding='pre')\n",
    "my_corpus_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  6,  7],\n",
       "       [ 3,  8,  4,  0],\n",
       "       [ 9, 10,  0,  0],\n",
       "       [ 5,  0,  0,  0],\n",
       "       [ 4,  5,  0,  0],\n",
       "       [ 3,  0,  0,  0],\n",
       "       [ 1,  2,  0,  0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Post\" padding을 적용해 본다.\n",
    "my_corpus_padded = pad_sequences(my_corpus_encoded, padding=\"post\")\n",
    "my_corpus_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6,  7],\n",
       "       [ 3,  8,  4],\n",
       "       [ 9, 10,  0],\n",
       "       [ 5,  0,  0],\n",
       "       [ 4,  5,  0],\n",
       "       [ 3,  0,  0],\n",
       "       [ 1,  2,  0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장의 길이에 제약을 둘 수도 있다.\n",
    "my_corpus_padded = pad_sequences(my_corpus_encoded, padding = 'post',maxlen=3)\n",
    "my_corpus_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  6,  7,  0,  0],\n",
       "       [ 3,  8,  4,  0,  0,  0],\n",
       "       [ 9, 10,  0,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  0],\n",
       "       [ 4,  5,  0,  0,  0,  0],\n",
       "       [ 3,  0,  0,  0,  0,  0],\n",
       "       [ 1,  2,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장의 길이를 늘릴 수도 있다.\n",
    "my_corpus_padded = pad_sequences(my_corpus_encoded, padding = 'post', maxlen = 6)\n",
    "my_corpus_padded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
